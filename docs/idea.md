我现在在尝试一种新的项目构建方式：文档先行，代码随后。

我会先把我的思路描述清楚，再开始编码。

那么第一个问题：我想干啥？

一直以来我都想构建一个跨设备的音乐播放生态，允许音频音乐在不同的设备间流转和协同播放。  
其中，协同播放这一块，不可避免的需要处理音轨对齐的问题。  
音频流通过网络传输，而网络延迟波动是不确定的。即使从服务端同步发送音频流，设备端也无法准确对齐播放。

因此，我们只能通过闭环的方式，即通过麦克风，在设备端感知延迟，回送到服务端进行延迟校准，把某些流需要延迟的，我们建立一个延迟缓冲区，把流数据先暂存在里边放几秒（算法计算得出），再放送到设备。

那么决定了，这个项目就叫——

# 延迟同步游乐场

我们循序渐进地进行项目规划、研发与测试。

虽然最终希望的是实现音频的跨设备同步，但图像的同步比音频同步要好测试的多。因为我们可以轻松的肉眼观察到两台设备谁快谁慢，而另一方面，音频，两台设备同时播放却不同步时，我们很难清楚判断谁快了、谁慢了。

## 第一阶段：图像同步

那么在第一阶段，我希望的是，制作一个相片播放器，能够在不同的设备间同步同时的进行相片切换。  
要实现第一阶段的目标，首先要实现的底层基础设施就是闭环图像延迟控制，也就是需要有摄像头，让我们的服务器能清楚知道每个设备的端到端延迟情况，再做出自动化的调整。  
那么我们提取出了一个关键功能：【自动延迟侦测器】，它的任务是输入摄像头视频流，分别输出两个（或许多个）设备的延迟。  

在实现“自动延迟侦测器”之前，我们先给它构建一个基础框架：

1. 一个服务端，用于分发要显示的相片
2. 一个 Webapp 端，用于显示相片
3. 服务端需要能够单独配置每个设备的延迟

这个基础框架应该能实现的效果是：我能够通过给服务端发指令，让多个设备的 Webapp 端显示指定相片。然后通过自行调整延迟，能够实现设备间同步同时切换相片（虽然这可能很不稳定）

那么创建这个项目吧！